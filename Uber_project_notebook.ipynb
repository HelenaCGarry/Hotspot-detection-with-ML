{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4a2dad-e2a6-4421-a4a3-b2512e20f364",
   "metadata": {},
   "source": [
    "## Hotspot detection with unsupervised ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee913e-08bc-469e-9cd6-273097c8c25f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Detecting Hotspots in the month of May 2014 in NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae3c1f-75b3-46ca-b7d1-746c98e50ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install geopy -q\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, accuracy_score, classification_report\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.datasets import load_digits\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297cadf4-6ec3-40ef-83a6-f5b56cb6d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"uber-raw-data-may14.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d85718-f02a-4cff-8548-90adc43b5986",
   "metadata": {},
   "source": [
    "- Base : the TLC base company code affiliated with the Uber pickup REMOVE\n",
    "- Date/Time: extract hour, day (year is not necessary, neither is the month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecafb46-425e-4d80-a798-95d659a10e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe347d4-54f2-4d91-bb6a-8a4c89bd71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date/Time'] = pd.to_datetime(df['Date/Time'],infer_datetime_format=True)\n",
    "df['Day_of_Week'] = df['Date/Time'].dt.dayofweek\n",
    "df['Hour'] = df['Date/Time'].dt.hour\n",
    "df_clean=df.drop(['Date/Time','Base'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3675fd-32a1-416d-917f-b2f5a109f4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = df.sample(1000)\n",
    "sample=sample.sort_values(\"Hour\", ascending=True)\n",
    "\n",
    "fig = px.density_mapbox(sample, lat = 'Lat', lon = 'Lon', \n",
    "                        mapbox_style=\"carto-positron\", zoom= 9, \n",
    "                        color_continuous_scale='viridis', opacity=0.6, radius=15)\n",
    "fig.update_layout(\n",
    "    width=1100,\n",
    "    height=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fc3ce-ceb1-4d7e-985f-befb7c6b273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_clean.sample(250000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65003928-c9db-49fe-86b4-c30c58092a18",
   "metadata": {},
   "source": [
    "## Calculating KMeans from the complete sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaeb78d-b379-4aea-96c5-4fdd4f6865de",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(sample)\n",
    "X_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da037b1d-0fe3-4822-b595-25fb46beaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "\n",
    "kmeans.fit(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4072f-16c2-45e9-b402-26eb467c836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_all = scaler.inverse_transform(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b117e92-a742-439b-ab7a-1b1e07f5bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=kmeans.predict(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8a5c9-6718-423a-a5b2-8efdd4124d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_all = sample.copy()\n",
    "df_clean_all['cluster_id'] = c\n",
    "fig_sample = df_clean_all.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef8c8c-56ca-49f4-84e9-6507214c4c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.scatter_mapbox(fig_sample, lat = 'Lat', lon = 'Lon', mapbox_style=\"carto-positron\", zoom= 10, color='cluster_id',width=1000,height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2901c531-eecf-4cc8-a82e-41b3d239862b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Creating clusters from the complete dataset doesn't appear to produce useful information about hotspots**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a881d7-b851-4896-a9c9-c01de490a50c",
   "metadata": {},
   "source": [
    "## Using DBScan to remove outliers before identifying pick-up hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5518d7f-8fca-41b3-a36b-ed2c8395668d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161fa3b4-94a0-496d-82d9-955b5015ef5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instanciate DBSCAN and fit\n",
    "db = DBSCAN(eps=0.20, min_samples=20)\n",
    "db.fit(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad907fa-7896-48fd-91a0-496b55ea27e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_db = sample.copy()\n",
    "sample_db['Cluster_id'] =  db.labels_\n",
    "sample_db['Cluster_id'].value_counts()/sample_db.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18119521-9851-495a-b686-c59be4aa87d4",
   "metadata": {},
   "source": [
    "We can aim at eliminating 10% of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181b0fe-94f1-464e-bbf1-83b5bb96d913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_db_f = sample_db.loc[sample_db['Cluster_id']>=0,['Lat','Lon','Day_of_Week','Hour']]\n",
    "print(sample_db_f.shape[0])\n",
    "sample_db_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8140a-842b-4639-bb91-89b2deb81dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the number of clusters is optimized\n",
    "# calculate inertia and silhouette score for 12h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2927f878-9bef-4948-9526-3d51d016fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a loop that will collect the Within-sum-of-square (wcss) for each value K \n",
    "# Let's use .inertia_ parameter to get the within sum of square value for each value K \n",
    "wcss =  []\n",
    "k = []\n",
    "h=12\n",
    "scaler_2 = StandardScaler()\n",
    "sample_db_f = sample_db_f.sort_values('Hour')\n",
    "X_h = sample_db_f.loc[sample_db_f['Hour']==h,['Lat','Lon']]\n",
    "X_h_n = scaler_2.fit_transform(X_h)\n",
    "\n",
    "\n",
    "for i in range (1,13): \n",
    "    kmeans = KMeans(n_clusters= i, random_state = 0)\n",
    "    kmeans.fit(X_h)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    k.append(i)\n",
    "    print(\"WCSS for K={} --> {}\".format(i, wcss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6757a4b-7cdc-4d17-aec7-7a9c0d8238c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize using plotly\n",
    "import plotly.express as px\n",
    "\n",
    "# Create DataFrame\n",
    "wcss_frame = pd.DataFrame(wcss)\n",
    "k_frame = pd.Series(k)\n",
    "\n",
    "# Create figure\n",
    "fig= px.line(\n",
    "    wcss_frame,\n",
    "    x=k_frame,\n",
    "    y=wcss_frame.iloc[:,-1]\n",
    ")\n",
    "\n",
    "# Create title and axis labels\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Inertia\",\n",
    "    xaxis_title=\"# Clusters\",\n",
    "    title=\"Inertia per cluster\"\n",
    ")\n",
    "\n",
    "# Render\n",
    "#fig.show(renderer=\"notebook\")\n",
    "fig.show(renderer=\"iframe\") # if using workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0dc15-3e89-46ae-a9c7-7fa88bf57d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Computer mean silhouette score\n",
    "sil = []\n",
    "k = []\n",
    "\n",
    "\n",
    "## Careful, you need to start at i=2 as silhouette score cannot accept less than 2 labels \n",
    "for i in range (2,13): \n",
    "    kmeans = KMeans(n_clusters= i, random_state = 0)\n",
    "    kmeans.fit(X_h_n)\n",
    "    sil.append(silhouette_score(X_h_n, kmeans.predict(X_h_n)))\n",
    "    k.append(i)\n",
    "    print(\"Silhouette score for K={} is {}\".format(i, sil[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f0db4-4edc-4e10-ad4c-01ca00d2ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame \n",
    "cluster_scores=pd.DataFrame(sil)\n",
    "k_frame = pd.Series(k)\n",
    "\n",
    "# Create figure\n",
    "fig = px.bar(data_frame=cluster_scores,  \n",
    "             x=k, \n",
    "             y=cluster_scores.iloc[:, -1]\n",
    "            )\n",
    "\n",
    "# Add title and axis labels\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Silhouette Score\",\n",
    "    xaxis_title=\"# Clusters\",\n",
    "    title=\"Silhouette Score per cluster\"\n",
    ")\n",
    "\n",
    "# Render\n",
    "#fig.show(renderer=\"notebook\")\n",
    "fig.show(renderer=\"iframe\") # if using workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33be667-ca94-4358-83bf-1a05d0dad1a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler_2 = StandardScaler()\n",
    "labels_hour=[]\n",
    "coord_hour={}\n",
    "cluster_size={}\n",
    "sample_db_f = sample_db_f.sort_values('Hour')\n",
    "for i in range(0,24):\n",
    "    # create sub data set to analyze by selecting by the hour and eliminating hour \n",
    "    X_i = sample_db_f.loc[sample_db_f['Hour']==i,['Lat','Lon']]\n",
    "    # normalize\n",
    "    X_i_n = scaler_2.fit_transform(X_i)\n",
    "    # train model\n",
    "    kmeans = KMeans(n_clusters=9, random_state=0)\n",
    "    kmeans.fit(X_i_n)    \n",
    "    # update dataframe with lat, lon, hour, and cluster#\n",
    "    labels = kmeans.labels_.tolist()\n",
    "    X_i['cluster_id'] = labels\n",
    "    coordinates = kmeans.cluster_centers_\n",
    "    coord_hour[i] = scaler_2.inverse_transform(coordinates)\n",
    "    cluster_size[i]= X_i['cluster_id'].value_counts(sort=False).sort_index().to_numpy()\n",
    "    labels_hour.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097758e-6847-4ca4-a271-42ee4212e320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusters = np.array([[0, 0, 0, 0]])\n",
    "for i in range(0,24):\n",
    "    cluster_data = np.concatenate([np.full((9, 1), i), coord_hour[i], cluster_size[i].reshape(9,1)], axis=1)\n",
    "    clusters = np.concatenate([clusters,cluster_data],axis=0)\n",
    "\n",
    "clusters=np.delete(clusters, 0, 0)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e3c59-7675-4ab9-a100-b26d132e5c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hour_hotspots = pd.DataFrame(clusters, columns = ['Hour','Lat', 'Lon','Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777cf71-084d-4673-838f-9f02a29d943e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "neighborhood=[]\n",
    "for lat,lon in hour_hotspots.loc[:,['Lat','Lon']].to_numpy():\n",
    "    geolocator = Nominatim(user_agent=\"NY_hotspots\")\n",
    "    location = geolocator.reverse(str(round(lat,3))+\", \"+str(round(lon,3)))\n",
    "    try:\n",
    "        neighborhood.append(location.raw['address']['neighbourhood'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        neighborhood.append(location.raw['address']['quarter'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try: \n",
    "        neighborhood.append(location.raw['address']['aeroway'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try: \n",
    "        neighborhood.append(location.raw['address']['amenity'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try: \n",
    "        neighborhood.append(location.raw['address']['suburb'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try: \n",
    "        neighborhood.append(location.raw['address']['commercial'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    neighborhood.append(str(lat)+\" \"+str(lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ec7a8-602e-439c-bed0-35eff6ba183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_hotspots['Location'] = neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b66dea-44c0-4067-b45a-19b02f3ace94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(hour_hotspots, lat = 'Lat', lon = 'Lon',size = 'Size',\n",
    "                  color = 'Size', hover_name= 'Location',\n",
    "                  hover_data={\"Size\": False,\n",
    "                              \"Hour\":False,\n",
    "                            \"Lat\": False,\n",
    "                            \"Lon\": False\n",
    "                        },\n",
    "                  animation_frame='Hour',\n",
    "                  size_max=50,\n",
    "                  mapbox_style=\"carto-positron\", zoom= 10, \n",
    "                  width=1000,height=700)\n",
    "\n",
    "fig.write_html(\"Hour_map.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8adfff-8eed-4cc6-85c8-311e9207347e",
   "metadata": {},
   "source": [
    "## Day-by-day pickup hotspots in NY using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1e413-9618-4647-9366-ae4a35d57ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_3 = StandardScaler()\n",
    "labels_week=[]\n",
    "coord_week={}\n",
    "cluster_size_week={}\n",
    "sample_db_f = sample_db_f.sort_values('Day_of_Week')\n",
    "for i in range(0,7):\n",
    "    # create sub data set to analyze by selecting by the hour and eliminating hour \n",
    "    X_i = sample_db_f.loc[sample_db_f['Day_of_Week']==i,['Lat','Lon']]\n",
    "    # normalize\n",
    "    X_i_n = scaler_3.fit_transform(X_i)\n",
    "    # train model\n",
    "    kmeans = KMeans(n_clusters=9, random_state=0)\n",
    "    kmeans.fit(X_i_n)    \n",
    "    # update dataframe with lat, lon, hour, and cluster#\n",
    "    labels = kmeans.labels_.tolist()\n",
    "    X_i['cluster_id'] = labels\n",
    "    coordinates = kmeans.cluster_centers_\n",
    "    coord_week[i] = scaler_3.inverse_transform(coordinates)\n",
    "    cluster_size_week[i]= X_i['cluster_id'].value_counts(sort=False).sort_index().to_numpy()\n",
    "    labels_week.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f214753-f340-41af-a2f5-6041bf47057f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusters_week = np.array([[0, 0, 0, 0]])\n",
    "for i in range(0,7):\n",
    "    cluster_data = np.concatenate([np.full((9, 1), i), coord_week[i], cluster_size_week[i].reshape(9,1)], axis=1)\n",
    "    clusters_week = np.concatenate([clusters_week,cluster_data],axis=0)\n",
    "\n",
    "clusters_week=np.delete(clusters_week, 0,0)\n",
    "np.set_printoptions(suppress=True)\n",
    "clusters_week[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6feb2-311b-4a28-9da5-c90f598864bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "week_hotspots = pd.DataFrame(clusters_week, columns = ['Day_of_Week','Lat', 'Lon','Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4add09-5b58-4476-affb-53a54f7cd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood=[]\n",
    "for lat,lon in week_hotspots.loc[:,['Lat','Lon']].to_numpy():\n",
    "    geolocator = Nominatim(user_agent=\"NY_hotspots\")\n",
    "    location = geolocator.reverse(str(round(lat,3))+\", \"+str(round(lon,3)))\n",
    "    try:\n",
    "        neighborhood.append(location.raw['address']['neighbourhood'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        neighborhood.append(location.raw['address']['quarter'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try: \n",
    "        neighborhood.append(location.raw['address']['aeroway'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try: \n",
    "        neighborhood.append(location.raw['address']['amenity'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try: \n",
    "        neighborhood.append(location.raw['address']['suburb'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try: \n",
    "        neighborhood.append(location.raw['address']['commercial'])\n",
    "        continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "    neighborhood.append(str(lat)+\" \"+str(lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c4b02f-8b4c-4fb3-a960-f8f73943ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_hotspots['Location'] = neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c523c-d659-44c2-bc2b-4807ab41ef8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.scatter_mapbox(week_hotspots, lat = 'Lat', lon = 'Lon',size = 'Size',\n",
    "                  color = 'Size',\n",
    "                  animation_frame='Day_of_Week',\n",
    "                  size_max=50,\n",
    "                  mapbox_style=\"carto-positron\", zoom= 10, \n",
    "                  width=1000,height=700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
